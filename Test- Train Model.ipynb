{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INCOMPLETE, NOT READY TO BE RUN\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from modeling_utils import NumpyDataGenerator\n",
    "\n",
    "\n",
    "# Parameters\n",
    "\n",
    "def train_model(metadata_location,\n",
    "                model_architecture = None,\n",
    "                data_dim=(128,128),\n",
    "                batch_size=64,\n",
    "                n_classes=10,\n",
    "                training_folds = [10,2,3,4,5,6,7,8,9],\n",
    "                validation_folds = [1],\n",
    "                shuffle=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    params = {'dim': data_dim,\n",
    "              'batch_size': batch_size,\n",
    "              'n_classes': n_classes,\n",
    "              'shuffle': shuffle}\n",
    "\n",
    "    # Datasets\n",
    "    metadata = pd.read_csv(metadata_location)\n",
    "    if shuffle:\n",
    "        metadata = metadata.sample(frac=1)\n",
    "    id_to_file_mapping = dict(zip(metadata['fsID'],metadata['location']))\n",
    "    train_data = metadata[metadata['fold'].isin(training_folds)]\n",
    "    test_data = metadata[metadata['fold'].isin(validation_folds)]\n",
    "    # Generators\n",
    "    training_generator = NumpyDataGenerator(list(train_data['fsID']), list(train_data['classID']),id_to_file_mapping, **params)\n",
    "    validation_generator = NumpyDataGenerator(list(test_data['fsID']), list(test_data['classID']), id_to_file_mapping, **params)\n",
    "\n",
    "    # Design model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(128,128), activation='relu',name='main_input'))\n",
    "    model.add(Dense(8, activation='relu',name='middle_layer'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes, activation='sigmoid',name='classifier'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # Train model on dataset\n",
    "    steps_per_epoch = np.ceil(len(metadata) / batch_size)\n",
    "    validation_steps = np.ceil(len(list(test_data['classID']))/batch_size)\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        workers=6,\n",
    "                        verbose=1,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps,\n",
    "                        epochs=1,\n",
    "                        shuffle=True\n",
    "                       )\n",
    "    #Validate\n",
    "    validation_generator_2 = NumpyDataGenerator(list(test_data['fsID']), list(test_data['classID']), id_to_file_mapping, **params)\n",
    "    \n",
    "    predictions = model.predict_generator(validation_generator_2,\n",
    "                                      steps = validation_steps,\n",
    "                                      verbose=True)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    cm = confusion_matrix(list(test_data['classID']), y_pred[:len(list(test_data['classID']))])\n",
    "    print(cm)\n",
    "    print(np.unique(np.array(y_pred), return_counts=True))\n",
    "    print(np.unique(np.array(test_data['classID']), return_counts=True))\n",
    "    return y_pred, test_data['classID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = \"/media/romulo/6237-3231/urban_sound_challenge/spectrograms_128_array/location_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 20:27:02.826447 139800350508864 deprecation_wrapper.py:119] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 20:27:02.849943 139800350508864 deprecation_wrapper.py:119] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 20:27:02.857152 139800350508864 deprecation_wrapper.py:119] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 20:27:02.962707 139800350508864 deprecation_wrapper.py:119] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 20:27:02.986527 139800350508864 deprecation_wrapper.py:119] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 20:27:02.993007 139800350508864 deprecation.py:323] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 20:27:03.146623 139800350508864 deprecation_wrapper.py:119] From /home/romulo/github/pyvenv/urbansounds8k/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (Dense)           (None, 128, 12)           1548      \n",
      "_________________________________________________________________\n",
      "middle_layer (Dense)         (None, 128, 8)            104       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 11,902\n",
      "Trainable params: 11,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "986.0\n",
      "Epoch 1/1\n",
      "986/986 [==============================] - 42s 43ms/step - loss: 0.3776 - acc: 0.8943 - val_loss: 0.3388 - val_acc: 0.8999\n",
      "101/101 [==============================] - 3s 31ms/step\n",
      "Confusion Matrix\n",
      "[[ 31   0   5   8 155 134   1 462   4   0]\n",
      " [  7   0   1   0  21  31   0  73   0   0]\n",
      " [ 34   1   3   7 127 157   0 469   2   0]\n",
      " [ 37   1   6   6 127 116   0 341   1   0]\n",
      " [ 28   0   4   6 105 135   0 418   1   0]\n",
      " [ 30   0   2   7 137 138   2 448   4   0]\n",
      " [  9   0   2   4  29  20   0 100   0   0]\n",
      " [ 36   0   4   5 155 177   0 542   0   0]\n",
      " [ 30   0   4   7 131 114   0 400   2   0]\n",
      " [ 48   1   4   6 127 160   0 452   2   0]]\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]), array([ 293,    3,   35,   56, 1125, 1193,    3, 3740,   16]))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([800, 133, 800, 635, 697, 768, 164, 919, 688, 800]))\n"
     ]
    }
   ],
   "source": [
    "train_model(metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
